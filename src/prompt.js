// Load environment variables from a .env file into process.env.
// This is a convenient way to manage secrets like API keys without hard-coding them.
import "dotenv/config";

// The commented-out line shows the original import for the raw Google GenAI SDK.
// We are not using this because it is not directly compatible with LangChain's Expression Language (LCEL).
// import { GoogleGenAI } from "@google/genai";

// Import the PromptTemplate class from LangChain's core library.
// This class is used to create and manage dynamic, reusable prompt templates.
import { PromptTemplate } from "@langchain/core/prompts";

// Import the specific LangChain integration for Google's GenAI models.
// `ChatGoogleGenerativeAI` is a LangChain-compatible `Runnable` that wraps the Gemini API.
// It allows us to use Gemini models within LangChain's ecosystem.
import { ChatGoogleGenerativeAI } from "@langchain/google-genai";

// Instantiate the LangChain wrapper for the Gemini model.
const llm = new ChatGoogleGenerativeAI({
  // Specify the desired Gemini model to use. "gemini-1.5-flash" is an example of a fast, efficient model.
  model: "gemini-1.5-flash", 
  // Pass the API key, which is retrieved from the environment variable loaded by "dotenv/config".
  apiKey: process.env.VITE_LLM_API_KEY,
});

// Define a template string for the prompt.
// This string contains a placeholder `{question}` which will be filled with the user's input.
// This design makes the prompt reusable for different questions.
const standAloneQuestionTemplate =
  "Given a question, convert it to a standalone question. question: {question} standalone question:";

// Create a PromptTemplate instance from the template string.
// `PromptTemplate.fromTemplate()` parses the string and identifies the `{question}` placeholder as an input variable.
const StandaloneQuestionPrompt = PromptTemplate.fromTemplate(
  standAloneQuestionTemplate
);

// Log the `StandaloneQuestionPrompt` object.
// This is for debugging and shows the structure of the prompt template before it's used to generate text.
// It will display the template string and the list of input variables (in this case, just 'question').
console.log(StandaloneQuestionPrompt);

// Create a chain using LangChain's Expression Language (LCEL).
// The `.pipe()` method connects the output of one component to the input of the next.
// Here, the output of `StandaloneQuestionPrompt` (a formatted prompt) is piped as the input to the `llm` (the Gemini model).
const StandAloneQuestionChain = StandaloneQuestionPrompt.pipe(llm);

// Log the chain object.
// This will display the internal structure of the chain, showing how the prompt and the model are connected.
console.log(StandAloneQuestionChain);

// Invoke the chain with a specific input.
// The `invoke()` method runs the entire chain. It takes an object with the required input variables.
// The `question` property is used to fill the `{question}` placeholder in the prompt template.
// The `await` keyword is used because the chain's execution is an asynchronous operation (it makes a network call to the Gemini API).
const response = await StandAloneQuestionChain.invoke({
  question:
    "What are the technical requirements for running Scrimba? I only have a very old laptop which is not that powerful.",
});

// Log the final response from the Gemini model.
// This will output the response generated by the model after it processes the input from the chain.
console.log(response);